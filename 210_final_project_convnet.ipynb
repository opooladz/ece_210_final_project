{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "210_final_project_convnet.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/opooladz/ece_210_final_project/blob/master/210_final_project_convnet.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "DeLbOeSmqguA",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "31d38a52-9569-481c-e83e-cebcf20fd5af"
      },
      "cell_type": "code",
      "source": [
        "# Import Libs \n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.special\n",
        "import scipy.io as sio\n",
        "import math\n",
        "#from  scipy.ndimage import convolve1d\n",
        "\n",
        "# Reloading any code written in external .py files.\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "%matplotlib inline"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The autoreload extension is already loaded. To reload it, use:\n",
            "  %reload_ext autoreload\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "ogt-kQ8x61dI",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Keras try model \n",
        "def try_model(model, data, name, epochs=200, verbose=1):  \n",
        "  model.compile(optimizer='adam',\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'])\n",
        "\n",
        "  model.summary()  \n",
        "  history = model.fit(data['x_train'], data['y_train'], \n",
        "            epochs=epochs, batch_size=64, verbose=verbose, \n",
        "            validation_data=(data['x_val'], data['y_val']))  \n",
        "  score = model.evaluate(data['x_test'], data['y_test'])\n",
        "  print('Test accuracy: {}'.format(score))\n",
        "  \n",
        "#   out_path = 'serialized_models/{}.h5'.format(name)  \n",
        "#   model.save(out_path)\n",
        "  \n",
        "#   with open('serialized_models/{}_history.pickle'.format(name), 'wb') as handle:\n",
        "#     pickle.dump(history.history, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
        "    \n",
        "  return history"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "a24-OWBXsGkM",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "# Comms Stuff Will add more later\n",
        "def qammod(m,y):\n",
        "  if(np.all(y>=m)):\n",
        "    print('qammod: all elements of X must be in range [0,M-1]')\n",
        "  \n",
        "  if (~np.all(y == np.fix(y) ) ):\n",
        "    print(\"qammod: all elements of X must be integers\");\n",
        "  \n",
        "  c = np.sqrt(m);\n",
        "  if( ~(np.all(c == np.fix(c)) and np.all(np.math.log(c, 2) == np.fix(np.math.log(c, 2))) ) ):\n",
        "    print(\"qammod: M must be a square of a power of 2\");\n",
        "  \n",
        "  b = -2*np.fmod(y,c) + c - 1.;\n",
        "  a = 2*np.floor(y *1./c) - c + 1.;\n",
        "  x = a + 1.*1j*b\n",
        "  # lets hold x in a np array([real,imag])\n",
        "  # x = np.array([a,1.*b]).T\n",
        "  return [x , y] \n",
        "\n",
        "def qammod2(m,size,trials):\n",
        "  \"\"\"\n",
        "  m - highest integer for the randint\n",
        "  \"\"\"\n",
        "  y = np.random.randint(m,size=(trials,size))\n",
        "  if(np.all(y>=m)):\n",
        "    print('qammod: all elements of X must be in range [0,M-1]')\n",
        "  \n",
        "  if (~np.all(y == np.fix(y) ) ):\n",
        "    print(\"qammod: all elements of X must be integers\");\n",
        "  \n",
        "  c = np.sqrt(m);\n",
        "  if( ~(np.all(c == np.fix(c)) and np.all(np.math.log(c, 2) == np.fix(np.math.log(c, 2))) ) ):\n",
        "    print(\"qammod: M must be a square of a power of 2\");\n",
        "  \n",
        "  b = -2*np.fmod(y,c) + c - 1.;\n",
        "  a = 2*np.floor(y *1./c) - c + 1.;\n",
        "  #x = a + 1.*1j*b\n",
        "  # lets hold x in a dp array([real,imag])\n",
        "  #x = np.array([a,1.*b])\n",
        "  x = np.dstack((a,b))\n",
        "  return [x , y]                        \n",
        "\n",
        "def channel2(m,sequenceLen,trials,Noise,SNRdB):\n",
        "  \n",
        "  [x,y] = qammod2(m,sequenceLen,trials)\n",
        "  h = [1, 0.5, 0.3, -0.5, 0.2, 0, 0.4]\n",
        "  #h = [1]\n",
        "  #print(x.shape)\n",
        "#   xh = np.empty((sequenceLen+len(h)-1,2))\n",
        "#   xh[:,0] = np.convolve(x[:,0],h)   \n",
        "#   xh[:,1] = np.convolve(x[:,1],h)   \n",
        "  #xh = convolve1d(x,h,axis = 1)\n",
        "  xh = np.apply_along_axis(lambda m: np.convolve(m,h,mode=\"full\"),axis = 1,arr=x)\n",
        "  #print(xh.shape)\n",
        "  SNR = 10**(SNRdB/10)\n",
        "  sym_noise_pow = np.var(xh)/SNR\n",
        "  sym_noise_scale_fact = np.sqrt(sym_noise_pow/2)\n",
        "  if(Noise):\n",
        "    AWGN = sym_noise_scale_fact*np.random.normal(1, size=xh.shape) \n",
        "    xh = xh + AWGN\n",
        "  #return [xhN.T , np.repeat(y,2) ]\n",
        "  return [xh ,y ]\n",
        "  \n",
        "                        \n",
        "data={}\n",
        "def generateData2(m,sequenceLen,trials,Noise,SNRdB):\n",
        "  [data['x_train'],data['y_train']] = channel2(m=m,sequenceLen=sequenceLen,trials=trials,Noise=Noise,SNRdB=SNRdB)\n",
        "  [data['x_val'],data['y_val']] = channel2(m=m,sequenceLen=sequenceLen,trials=int(trials*0.5),Noise= Noise,SNRdB=SNRdB)\n",
        "  [data['x_test'],data['y_test']] = channel2(m=m,sequenceLen=sequenceLen,trials=trials,Noise=Noise,SNRdB=SNRdB)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sztUIJEosHZQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "8b456c47-8e8b-419f-b6a7-9105927ae4b1"
      },
      "cell_type": "code",
      "source": [
        "# Visulizing the data\n",
        "\n",
        "m = 4         # order of modulation\n",
        "#seqLen = 64    # length of one trial must be square powers of 2 to make sense see below \n",
        "trials = 1000   # number of trials \n",
        "Noise = 1\n",
        "SNRdB = 25 \n",
        "\n",
        "\n",
        "x = 4\n",
        "square_power_of_two = 2**(2*x)\n",
        "seqLen = square_power_of_two  \n",
        "print(seqLen)\n",
        "\n",
        "generateData2(m,seqLen,trials,Noise,SNRdB)\n",
        "print(data['x_train'].shape) # data  # axis 1 should be 2x seqLen because of conv with a 2tap filter\n",
        "print(data['y_train'].shape) # labels\n",
        "#print(data['x_train']) # so u can see the data \n",
        "\n",
        "# tmp = (data['x_train'][:,:-1].reshape(trials,seqLen))\n",
        "\n",
        "# #plot 1\n",
        "# plt.scatter(np.real(tmp[0][:]),np.imag(tmp[0][:]))   \n",
        "#                                          # I only plot the first 10 points from \n",
        "#                                          # the sequence because as time goes on \n",
        "#                                          # the data becomes less dependent on the\n",
        "#                                          # previous symbols. Even 10 is a lot. \n",
        "# plt.title('4-QAM Mod With Multi-Channel AWGN plot 1')\n",
        "# plt.show()\n",
        "\n",
        "#plot 2\n",
        "# Just a pure 16 qam with 1 point on each constalation point \n",
        "# clearly we have 16 classes\n",
        "# [xtmp,ytmp]=  qammod(16,data['y_train'])\n",
        "# plt.scatter(xtmp[:,0],xtmp[:,1])\n",
        "# plt.title('16-QAM Mod plot 2')\n",
        "# plt.show()\n",
        "\n"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "256\n",
            "(1000, 262, 2)\n",
            "(1000, 256)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "p6RLVcKzsKTG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "outputId": "8a758dab-2834-4235-d981-abb9e163de68"
      },
      "cell_type": "code",
      "source": [
        "# Some keras stuffs\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, Activation, Conv2D, Flatten, AveragePooling2D, MaxPooling2D, Dropout, BatchNormalization\n",
        "from keras.layers import Reshape, Permute\n",
        "from keras import regularizers\n",
        "import keras\n",
        "ykeys = {'y_train','y_val','y_test'}\n",
        "xkeys = {'x_train','x_val','x_test'}\n",
        "\n",
        "  \n",
        "y_train = keras.utils.to_categorical(data['y_train'] , num_classes=m)\n",
        "y_val = keras.utils.to_categorical(data['y_val'] , num_classes=m)\n",
        "y_test = keras.utils.to_categorical(data['y_test'] , num_classes=m)\n",
        "\n",
        "print(data['x_train'].shape)\n",
        "print(data['x_test'].shape)\n",
        "print(y_train.shape)\n",
        "print(data['x_val'].shape)\n",
        "\n",
        "\n",
        "default_data = {\n",
        "  'x_train': data['x_train'],\n",
        "  'x_val': data['x_val'],\n",
        "  'x_test': data['x_test'],\n",
        "  'y_train': y_train,\n",
        "  'y_val': y_val,\n",
        "  'y_test': y_test,\n",
        "}\n"
      ],
      "execution_count": 109,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1000, 262, 2)\n",
            "(1000, 262, 2)\n",
            "(1000, 256, 4)\n",
            "(500, 262, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "5Jz3iVxn48iq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3875
        },
        "outputId": "d88eed79-7c99-4f27-88bf-6cfd89e48881"
      },
      "cell_type": "code",
      "source": [
        "from keras.layers import Reshape,Dense,Embedding,Conv1D, LSTM,CuDNNLSTM, TimeDistributed, GaussianNoise, Softmax,Flatten\n",
        "from keras import regularizers\n",
        "\n",
        "modCNN = Sequential()\n",
        "modCNN.add(Conv1D(40,kernel_size=(40), input_shape=(seqLen+6,2) ))\n",
        "modCNN.add(Flatten())\n",
        "modCNN.add(Dense(seqLen*m,kernel_regularizer=regularizers.l2(0.01)))\n",
        "modCNN.add(Reshape(target_shape = (seqLen,m)))\n",
        "modCNN.add(Softmax(axis=2))\n",
        "try_model(modCNN, default_data, 'conv_lstmx', epochs=100)\n"
      ],
      "execution_count": 118,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "conv1d_29 (Conv1D)           (None, 223, 40)           3240      \n",
            "_________________________________________________________________\n",
            "flatten_14 (Flatten)         (None, 8920)              0         \n",
            "_________________________________________________________________\n",
            "dense_21 (Dense)             (None, 1024)              9135104   \n",
            "_________________________________________________________________\n",
            "reshape_20 (Reshape)         (None, 256, 4)            0         \n",
            "_________________________________________________________________\n",
            "softmax_15 (Softmax)         (None, 256, 4)            0         \n",
            "=================================================================\n",
            "Total params: 9,138,344\n",
            "Trainable params: 9,138,344\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Train on 1000 samples, validate on 500 samples\n",
            "Epoch 1/100\n",
            "1000/1000 [==============================] - 1s 1ms/step - loss: 10.0399 - acc: 0.3357 - val_loss: 2.9140 - val_acc: 0.4825\n",
            "Epoch 2/100\n",
            "1000/1000 [==============================] - 0s 364us/step - loss: 1.8574 - acc: 0.6200 - val_loss: 1.5436 - val_acc: 0.5734\n",
            "Epoch 3/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 1.4239 - acc: 0.6830 - val_loss: 1.3987 - val_acc: 0.5873\n",
            "Epoch 4/100\n",
            "1000/1000 [==============================] - 0s 373us/step - loss: 1.2694 - acc: 0.6763 - val_loss: 1.3038 - val_acc: 0.5981\n",
            "Epoch 5/100\n",
            "1000/1000 [==============================] - 0s 368us/step - loss: 1.2348 - acc: 0.6713 - val_loss: 1.2985 - val_acc: 0.6034\n",
            "Epoch 6/100\n",
            "1000/1000 [==============================] - 0s 364us/step - loss: 1.2406 - acc: 0.6701 - val_loss: 1.3005 - val_acc: 0.6126\n",
            "Epoch 7/100\n",
            "1000/1000 [==============================] - 0s 373us/step - loss: 1.2285 - acc: 0.6818 - val_loss: 1.2917 - val_acc: 0.6207\n",
            "Epoch 8/100\n",
            "1000/1000 [==============================] - 0s 363us/step - loss: 1.2281 - acc: 0.6840 - val_loss: 1.2913 - val_acc: 0.6271\n",
            "Epoch 9/100\n",
            "1000/1000 [==============================] - 0s 375us/step - loss: 1.1864 - acc: 0.7057 - val_loss: 1.2775 - val_acc: 0.6336\n",
            "Epoch 10/100\n",
            "  64/1000 [>.............................] - ETA: 0s - loss: 0.9334 - acc: 0.8506"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 379us/step - loss: 1.1854 - acc: 0.7051 - val_loss: 1.2752 - val_acc: 0.6395\n",
            "Epoch 11/100\n",
            "1000/1000 [==============================] - 0s 362us/step - loss: 1.1697 - acc: 0.7159 - val_loss: 1.2668 - val_acc: 0.6460\n",
            "Epoch 12/100\n",
            "1000/1000 [==============================] - 0s 357us/step - loss: 1.1747 - acc: 0.7168 - val_loss: 1.2703 - val_acc: 0.6477\n",
            "Epoch 13/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 1.1664 - acc: 0.7224 - val_loss: 1.2586 - val_acc: 0.6571\n",
            "Epoch 14/100\n",
            "1000/1000 [==============================] - 0s 362us/step - loss: 1.1665 - acc: 0.7239 - val_loss: 1.2601 - val_acc: 0.6579\n",
            "Epoch 15/100\n",
            "1000/1000 [==============================] - 0s 360us/step - loss: 1.1700 - acc: 0.7271 - val_loss: 1.2566 - val_acc: 0.6659\n",
            "Epoch 16/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 1.1402 - acc: 0.7389 - val_loss: 1.2391 - val_acc: 0.6690\n",
            "Epoch 17/100\n",
            "1000/1000 [==============================] - 0s 374us/step - loss: 1.1185 - acc: 0.7460 - val_loss: 1.2269 - val_acc: 0.6765\n",
            "Epoch 18/100\n",
            "1000/1000 [==============================] - 0s 376us/step - loss: 1.1120 - acc: 0.7502 - val_loss: 1.2247 - val_acc: 0.6793\n",
            "Epoch 19/100\n",
            "1000/1000 [==============================] - 0s 367us/step - loss: 1.1113 - acc: 0.7520 - val_loss: 1.2284 - val_acc: 0.6802\n",
            "Epoch 20/100\n",
            "1000/1000 [==============================] - 0s 365us/step - loss: 1.1173 - acc: 0.7513 - val_loss: 1.2252 - val_acc: 0.6833\n",
            "Epoch 21/100\n",
            "1000/1000 [==============================] - 0s 372us/step - loss: 1.1199 - acc: 0.7518 - val_loss: 1.2261 - val_acc: 0.6870\n",
            "Epoch 22/100\n",
            "1000/1000 [==============================] - 0s 371us/step - loss: 1.1079 - acc: 0.7573 - val_loss: 1.2180 - val_acc: 0.6864\n",
            "Epoch 23/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 1.1129 - acc: 0.7568 - val_loss: 1.2184 - val_acc: 0.6929\n",
            "Epoch 24/100\n",
            "1000/1000 [==============================] - 0s 365us/step - loss: 1.0898 - acc: 0.7670 - val_loss: 1.2050 - val_acc: 0.6942\n",
            "Epoch 25/100\n",
            " 256/1000 [======>.......................] - ETA: 0s - loss: 0.8654 - acc: 0.8739"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 368us/step - loss: 1.0803 - acc: 0.7672 - val_loss: 1.2011 - val_acc: 0.6958\n",
            "Epoch 26/100\n",
            "1000/1000 [==============================] - 0s 369us/step - loss: 1.0724 - acc: 0.7711 - val_loss: 1.1935 - val_acc: 0.6988\n",
            "Epoch 27/100\n",
            "1000/1000 [==============================] - 0s 365us/step - loss: 1.0738 - acc: 0.7709 - val_loss: 1.1929 - val_acc: 0.7011\n",
            "Epoch 28/100\n",
            "1000/1000 [==============================] - 0s 360us/step - loss: 1.0709 - acc: 0.7726 - val_loss: 1.1925 - val_acc: 0.7016\n",
            "Epoch 29/100\n",
            "1000/1000 [==============================] - 0s 362us/step - loss: 1.0703 - acc: 0.7737 - val_loss: 1.1904 - val_acc: 0.7023\n",
            "Epoch 30/100\n",
            "1000/1000 [==============================] - 0s 363us/step - loss: 1.0566 - acc: 0.7805 - val_loss: 1.1889 - val_acc: 0.7019\n",
            "Epoch 31/100\n",
            "1000/1000 [==============================] - 0s 376us/step - loss: 1.0487 - acc: 0.7812 - val_loss: 1.1773 - val_acc: 0.7061\n",
            "Epoch 32/100\n",
            "1000/1000 [==============================] - 0s 365us/step - loss: 1.0349 - acc: 0.7851 - val_loss: 1.1675 - val_acc: 0.7102\n",
            "Epoch 33/100\n",
            "1000/1000 [==============================] - 0s 370us/step - loss: 1.0428 - acc: 0.7835 - val_loss: 1.1710 - val_acc: 0.7112\n",
            "Epoch 34/100\n",
            "1000/1000 [==============================] - 0s 367us/step - loss: 1.0198 - acc: 0.7924 - val_loss: 1.1578 - val_acc: 0.7139\n",
            "Epoch 35/100\n",
            "1000/1000 [==============================] - 0s 372us/step - loss: 1.0106 - acc: 0.7935 - val_loss: 1.1560 - val_acc: 0.7142\n",
            "Epoch 36/100\n",
            "1000/1000 [==============================] - 0s 368us/step - loss: 1.0032 - acc: 0.7959 - val_loss: 1.1536 - val_acc: 0.7145\n",
            "Epoch 37/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.9957 - acc: 0.7984 - val_loss: 1.1474 - val_acc: 0.7164\n",
            "Epoch 38/100\n",
            "1000/1000 [==============================] - 0s 363us/step - loss: 1.0054 - acc: 0.7956 - val_loss: 1.1542 - val_acc: 0.7172\n",
            "Epoch 39/100\n",
            "1000/1000 [==============================] - 0s 365us/step - loss: 0.9852 - acc: 0.8051 - val_loss: 1.1451 - val_acc: 0.7173\n",
            "Epoch 40/100\n",
            " 256/1000 [======>.......................] - ETA: 0s - loss: 0.7937 - acc: 0.8928"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 365us/step - loss: 0.9768 - acc: 0.8040 - val_loss: 1.1368 - val_acc: 0.7190\n",
            "Epoch 41/100\n",
            "1000/1000 [==============================] - 0s 359us/step - loss: 0.9663 - acc: 0.8075 - val_loss: 1.1389 - val_acc: 0.7199\n",
            "Epoch 42/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.9654 - acc: 0.8082 - val_loss: 1.1321 - val_acc: 0.7214\n",
            "Epoch 43/100\n",
            "1000/1000 [==============================] - 0s 361us/step - loss: 0.9566 - acc: 0.8110 - val_loss: 1.1254 - val_acc: 0.7214\n",
            "Epoch 44/100\n",
            "1000/1000 [==============================] - 0s 364us/step - loss: 0.9407 - acc: 0.8160 - val_loss: 1.1218 - val_acc: 0.7239\n",
            "Epoch 45/100\n",
            "1000/1000 [==============================] - 0s 368us/step - loss: 0.9444 - acc: 0.8131 - val_loss: 1.1188 - val_acc: 0.7246\n",
            "Epoch 46/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.9363 - acc: 0.8182 - val_loss: 1.1131 - val_acc: 0.7253\n",
            "Epoch 47/100\n",
            "1000/1000 [==============================] - 0s 363us/step - loss: 0.9279 - acc: 0.8198 - val_loss: 1.1095 - val_acc: 0.7271\n",
            "Epoch 48/100\n",
            "1000/1000 [==============================] - 0s 368us/step - loss: 0.9160 - acc: 0.8230 - val_loss: 1.1033 - val_acc: 0.7281\n",
            "Epoch 49/100\n",
            "1000/1000 [==============================] - 0s 353us/step - loss: 0.9105 - acc: 0.8242 - val_loss: 1.1002 - val_acc: 0.7306\n",
            "Epoch 50/100\n",
            "1000/1000 [==============================] - 0s 369us/step - loss: 0.8955 - acc: 0.8298 - val_loss: 1.0979 - val_acc: 0.7281\n",
            "Epoch 51/100\n",
            "1000/1000 [==============================] - 0s 367us/step - loss: 0.8737 - acc: 0.8356 - val_loss: 1.0828 - val_acc: 0.7300\n",
            "Epoch 52/100\n",
            "1000/1000 [==============================] - 0s 370us/step - loss: 0.8656 - acc: 0.8361 - val_loss: 1.0699 - val_acc: 0.7356\n",
            "Epoch 53/100\n",
            "1000/1000 [==============================] - 0s 370us/step - loss: 0.8726 - acc: 0.8338 - val_loss: 1.0818 - val_acc: 0.7337\n",
            "Epoch 54/100\n",
            "1000/1000 [==============================] - 0s 364us/step - loss: 0.8637 - acc: 0.8382 - val_loss: 1.0770 - val_acc: 0.7340\n",
            "Epoch 55/100\n",
            " 256/1000 [======>.......................] - ETA: 0s - loss: 0.6878 - acc: 0.9197"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.8608 - acc: 0.8378 - val_loss: 1.0728 - val_acc: 0.7359\n",
            "Epoch 56/100\n",
            "1000/1000 [==============================] - 0s 379us/step - loss: 0.8591 - acc: 0.8386 - val_loss: 1.0723 - val_acc: 0.7351\n",
            "Epoch 57/100\n",
            "1000/1000 [==============================] - 0s 361us/step - loss: 0.8557 - acc: 0.8398 - val_loss: 1.0767 - val_acc: 0.7347\n",
            "Epoch 58/100\n",
            "1000/1000 [==============================] - 0s 358us/step - loss: 0.8405 - acc: 0.8455 - val_loss: 1.0607 - val_acc: 0.7383\n",
            "Epoch 59/100\n",
            "1000/1000 [==============================] - 0s 359us/step - loss: 0.8329 - acc: 0.8453 - val_loss: 1.0586 - val_acc: 0.7381\n",
            "Epoch 60/100\n",
            "1000/1000 [==============================] - 0s 369us/step - loss: 0.8185 - acc: 0.8519 - val_loss: 1.0498 - val_acc: 0.7399\n",
            "Epoch 61/100\n",
            "1000/1000 [==============================] - 0s 374us/step - loss: 0.8129 - acc: 0.8513 - val_loss: 1.0458 - val_acc: 0.7399\n",
            "Epoch 62/100\n",
            "1000/1000 [==============================] - 0s 367us/step - loss: 0.8143 - acc: 0.8506 - val_loss: 1.0509 - val_acc: 0.7396\n",
            "Epoch 63/100\n",
            "1000/1000 [==============================] - 0s 368us/step - loss: 0.8107 - acc: 0.8516 - val_loss: 1.0495 - val_acc: 0.7415\n",
            "Epoch 64/100\n",
            "1000/1000 [==============================] - 0s 370us/step - loss: 0.8040 - acc: 0.8548 - val_loss: 1.0475 - val_acc: 0.7403\n",
            "Epoch 65/100\n",
            "1000/1000 [==============================] - 0s 367us/step - loss: 0.7889 - acc: 0.8596 - val_loss: 1.0372 - val_acc: 0.7404\n",
            "Epoch 66/100\n",
            "1000/1000 [==============================] - 0s 369us/step - loss: 0.7836 - acc: 0.8588 - val_loss: 1.0310 - val_acc: 0.7425\n",
            "Epoch 67/100\n",
            "1000/1000 [==============================] - 0s 365us/step - loss: 0.7871 - acc: 0.8576 - val_loss: 1.0367 - val_acc: 0.7422\n",
            "Epoch 68/100\n",
            "1000/1000 [==============================] - 0s 364us/step - loss: 0.7779 - acc: 0.8622 - val_loss: 1.0313 - val_acc: 0.7431\n",
            "Epoch 69/100\n",
            "1000/1000 [==============================] - 0s 364us/step - loss: 0.7740 - acc: 0.8616 - val_loss: 1.0267 - val_acc: 0.7439\n",
            "Epoch 70/100\n",
            " 256/1000 [======>.......................] - ETA: 0s - loss: 0.6318 - acc: 0.9271"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.7677 - acc: 0.8633 - val_loss: 1.0287 - val_acc: 0.7438\n",
            "Epoch 71/100\n",
            "1000/1000 [==============================] - 0s 373us/step - loss: 0.7601 - acc: 0.8654 - val_loss: 1.0213 - val_acc: 0.7474\n",
            "Epoch 72/100\n",
            "1000/1000 [==============================] - 0s 373us/step - loss: 0.7640 - acc: 0.8653 - val_loss: 1.0256 - val_acc: 0.7448\n",
            "Epoch 73/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.7620 - acc: 0.8651 - val_loss: 1.0283 - val_acc: 0.7440\n",
            "Epoch 74/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.7535 - acc: 0.8682 - val_loss: 1.0191 - val_acc: 0.7459\n",
            "Epoch 75/100\n",
            "1000/1000 [==============================] - 0s 371us/step - loss: 0.7481 - acc: 0.8692 - val_loss: 1.0188 - val_acc: 0.7465\n",
            "Epoch 76/100\n",
            "1000/1000 [==============================] - 0s 369us/step - loss: 0.7370 - acc: 0.8726 - val_loss: 1.0141 - val_acc: 0.7460\n",
            "Epoch 77/100\n",
            "1000/1000 [==============================] - 0s 377us/step - loss: 0.7372 - acc: 0.8710 - val_loss: 1.0094 - val_acc: 0.7482\n",
            "Epoch 78/100\n",
            "1000/1000 [==============================] - 0s 367us/step - loss: 0.7396 - acc: 0.8708 - val_loss: 1.0153 - val_acc: 0.7475\n",
            "Epoch 79/100\n",
            "1000/1000 [==============================] - 0s 369us/step - loss: 0.7375 - acc: 0.8719 - val_loss: 1.0137 - val_acc: 0.7473\n",
            "Epoch 80/100\n",
            "1000/1000 [==============================] - 0s 362us/step - loss: 0.7266 - acc: 0.8768 - val_loss: 1.0101 - val_acc: 0.7479\n",
            "Epoch 81/100\n",
            "1000/1000 [==============================] - 0s 374us/step - loss: 0.7238 - acc: 0.8762 - val_loss: 1.0056 - val_acc: 0.7482\n",
            "Epoch 82/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.7202 - acc: 0.8758 - val_loss: 0.9998 - val_acc: 0.7518\n",
            "Epoch 83/100\n",
            "1000/1000 [==============================] - 0s 362us/step - loss: 0.7052 - acc: 0.8817 - val_loss: 0.9929 - val_acc: 0.7513\n",
            "Epoch 84/100\n",
            "1000/1000 [==============================] - 0s 368us/step - loss: 0.7006 - acc: 0.8819 - val_loss: 0.9887 - val_acc: 0.7526\n",
            "Epoch 85/100\n",
            " 256/1000 [======>.......................] - ETA: 0s - loss: 0.5660 - acc: 0.9435"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 374us/step - loss: 0.6948 - acc: 0.8828 - val_loss: 0.9892 - val_acc: 0.7509\n",
            "Epoch 86/100\n",
            "1000/1000 [==============================] - 0s 362us/step - loss: 0.6927 - acc: 0.8832 - val_loss: 0.9899 - val_acc: 0.7523\n",
            "Epoch 87/100\n",
            "1000/1000 [==============================] - 0s 365us/step - loss: 0.6896 - acc: 0.8848 - val_loss: 0.9860 - val_acc: 0.7514\n",
            "Epoch 88/100\n",
            "1000/1000 [==============================] - 0s 377us/step - loss: 0.6839 - acc: 0.8853 - val_loss: 0.9876 - val_acc: 0.7512\n",
            "Epoch 89/100\n",
            "1000/1000 [==============================] - 0s 362us/step - loss: 0.6915 - acc: 0.8825 - val_loss: 0.9869 - val_acc: 0.7538\n",
            "Epoch 90/100\n",
            "1000/1000 [==============================] - 0s 371us/step - loss: 0.6906 - acc: 0.8846 - val_loss: 0.9939 - val_acc: 0.7520\n",
            "Epoch 91/100\n",
            "1000/1000 [==============================] - 0s 369us/step - loss: 0.6856 - acc: 0.8858 - val_loss: 0.9874 - val_acc: 0.7535\n",
            "Epoch 92/100\n",
            "1000/1000 [==============================] - 0s 374us/step - loss: 0.6828 - acc: 0.8857 - val_loss: 0.9905 - val_acc: 0.7519\n",
            "Epoch 93/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.6824 - acc: 0.8866 - val_loss: 0.9913 - val_acc: 0.7521\n",
            "Epoch 94/100\n",
            "1000/1000 [==============================] - 0s 365us/step - loss: 0.6723 - acc: 0.8897 - val_loss: 0.9806 - val_acc: 0.7541\n",
            "Epoch 95/100\n",
            "1000/1000 [==============================] - 0s 369us/step - loss: 0.6641 - acc: 0.8914 - val_loss: 0.9783 - val_acc: 0.7536\n",
            "Epoch 96/100\n",
            "1000/1000 [==============================] - 0s 373us/step - loss: 0.6594 - acc: 0.8925 - val_loss: 0.9735 - val_acc: 0.7547\n",
            "Epoch 97/100\n",
            "1000/1000 [==============================] - 0s 371us/step - loss: 0.6539 - acc: 0.8928 - val_loss: 0.9688 - val_acc: 0.7570\n",
            "Epoch 98/100\n",
            "1000/1000 [==============================] - 0s 366us/step - loss: 0.6621 - acc: 0.8902 - val_loss: 0.9765 - val_acc: 0.7546\n",
            "Epoch 99/100\n",
            "1000/1000 [==============================] - 0s 368us/step - loss: 0.6603 - acc: 0.8917 - val_loss: 0.9777 - val_acc: 0.7546\n",
            "Epoch 100/100\n",
            "  64/1000 [>.............................] - ETA: 0s - loss: 0.5066 - acc: 0.9659"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1000/1000 [==============================] - 0s 368us/step - loss: 0.6589 - acc: 0.8929 - val_loss: 0.9772 - val_acc: 0.7554\n",
            "1000/1000 [==============================] - 0s 159us/step\n",
            "Test accuracy: [0.9710390005111694, 0.75834765625]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7f8ea64c6860>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 118
        }
      ]
    },
    {
      "metadata": {
        "id": "QZ4ftV1IB2So",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}